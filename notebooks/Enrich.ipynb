{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Enrich Cells to be Run Through Trained OSM Completeness Model</h1>\n",
    "<p>After training an OSM completeness model and generating a collection of 250-m by 250-m cells over a region of interest, load that collection into this notebook to join features (e.g., VIIRS nighttime lights, NDVI, WSF) to it. After this notebook is finished, the output can be run through the trained OSM completeness model in the ApplyModel notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterstats import zonal_stats, utils\n",
    "import gdal, gdalconst\n",
    "import json\n",
    "from shapely.geometry import shape, box, mapping, Point, Polygon\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import math\n",
    "import osmnx as ox\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>These are all variables that need to be set before running the notebook</h2>\n",
    "<p>Download <a href =\"https://data.humdata.org/search?organization=facebook&q=high%20resolution%20population&ext_page_size=25&sort=score%20desc%2C%20if(gt(last_modified%2Creview_date)%2Clast_modified%2Creview_date)%20desc\">Facebook population data</a> and/or <a href=\"https://www.worldpop.org/geodata/listing?id=29\">WorldPop</a> data for your area of interest and insert the paths into the cell below. Facebook data is not available for every country, and the World Settlement Footprint (WSF) dataset is not publicly available. If not available, references to those datasets will have to be commented out.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbPath = 'path_to_facebook'\n",
    "wpPath = 'path_to_worldpop'\n",
    "wsfPath = 'path_to_wsf'\n",
    "\n",
    "#Change file name to point to file containing polygons from SplitArea notebook\n",
    "#In testing, a file of 440000 features took 4 hours to complete\n",
    "filename=\"path_to_SplitArea_output.json\"\n",
    "\n",
    "outputFilename = \"path_to_enriched_output.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for GeoJSON feature collections. To complete analysis need two:\n",
    "#1. training data from Observable notebook\n",
    "#2. grid data that wil be run through trained model and output\n",
    "class Features:\n",
    "    \n",
    "    def __init__(self, filename=''):\n",
    "        self.filename = filename\n",
    "        self.polygons = []\n",
    "        \n",
    "        if filename=='':\n",
    "            return\n",
    "        \n",
    "        with open(filename) as f:\n",
    "            self.data = json.load(f)\n",
    "        try:\n",
    "            #uncomment to do a quick test with only 100 features\n",
    "            #self.features = self.data['features'][0:100]\n",
    "            self.features = self.data['features']\n",
    "        except:\n",
    "            print(\"JSON file did not have features array\")\n",
    "            \n",
    "    def getPolygons(self):\n",
    "        for feature in self.features:\n",
    "            g = shape(feature['geometry']).buffer(0)\n",
    "            self.polygons.append(g)\n",
    "        print(len(self.polygons))\n",
    "        return self.polygons\n",
    "    \n",
    "    #used if creating own grid, not reading JSON file in __init__\n",
    "    def setData(self, df):\n",
    "        self.data = mapping(df.geometry)\n",
    "        #don't need bbox for anything so delete it to reduce output file size\n",
    "        del self.data['bbox']\n",
    "        for feature in self.data['features']:\n",
    "            del feature['bbox']\n",
    "        self.features = self.data['features']\n",
    "    \n",
    "    def projectGeoDataFrame(self, *args):\n",
    "        if len(args) > 0:\n",
    "            df = args[0]\n",
    "        else:\n",
    "            df = self.geoDataFrame\n",
    "        \n",
    "        avg_longitude = (self.bounds[0] + self.bounds[2])/2\n",
    "        utm_zone = int(math.floor((avg_longitude + 180) / 6.) + 1)\n",
    "        utm_crs = f'+proj=utm +zone={utm_zone} +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "   \n",
    "        # project the GeoDataFrame to the UTM CRS\n",
    "        self.geoDataFrameProjected = df.to_crs(utm_crs)\n",
    "    \n",
    "        return self.geoDataFrameProjected\n",
    "    \n",
    "    def createGeoDataFrame(self):\n",
    "        self.geoDataFrame = gpd.GeoDataFrame({\n",
    "            'geometry': self.polygons\n",
    "        })\n",
    "        self.geoDataFrame.crs = \"EPSG:4326\"\n",
    "        return self.geoDataFrame\n",
    "        \n",
    "    def getBounds(self):\n",
    "        self.bounds = self.geoDataFrame.total_bounds\n",
    "        return self.bounds\n",
    "    \n",
    "    def getClippingBox(self):\n",
    "        if not hasattr(self, 'bounds'):\n",
    "            self.getBounds()\n",
    "        bbox = box(self.bounds[0], self.bounds[1], self.bounds[2], self.bounds[3])\n",
    "        self.bbox = bbox\n",
    "        self.clippingBox = {'type': 'Feature', 'properties': {}, 'geometry': mapping(bbox)}\n",
    "        return self.clippingBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raster:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "    def dataFromBounds(self, band, bounds):\n",
    "        with rasterio.open(self.filename) as src:\n",
    "            self.noData = src.nodatavals[0]\n",
    "            window = rasterio.windows.from_bounds(bounds[0],bounds[1],bounds[2],bounds[3], src.transform)\n",
    "            self.boundsData = src.read(band, window=window)\n",
    "            self.boundsTransform = src.window_transform(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnClippedLength(spatialIndex, DF, clippingPoly):\n",
    "    possible_matches_index = list(spatialIndex.intersection(clippingPoly.bounds))\n",
    "    possible_matches = DF.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(clippingPoly)]\n",
    "    if (precise_matches.empty):\n",
    "        return 0\n",
    "    preciseClip = gpd.clip(precise_matches, clippingPoly)\n",
    "    return (preciseClip.length).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnClippedPoints(spatialIndex, DF, clippingPoly):\n",
    "    possible_matches_index = list(spatialIndex.intersection(clippingPoly.bounds))\n",
    "    possible_matches = DF.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(clippingPoly)]\n",
    "    if (precise_matches.empty):\n",
    "        return 0\n",
    "    preciseClip = gpd.clip(precise_matches, clippingPoly)\n",
    "    return len(preciseClip.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnClippedArea(spatialIndex, buildingDF, clippingPoly):\n",
    "    possible_matches_index = list(spatialIndex.intersection(clippingPoly.bounds))\n",
    "    possible_matches = buildingDF.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(clippingPoly)]\n",
    "    if (precise_matches.empty):\n",
    "        return 0\n",
    "    preciseClip = gpd.clip(precise_matches, clippingPoly)\n",
    "    return (preciseClip.area).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeForest(features):\n",
    "\n",
    "    ee.Initialize()\n",
    "    gfcImage = ee.Image(\"UMD/hansen/global_forest_change_2018_v1_6\")\n",
    "    FCL0018 = gfcImage.select(['loss']).eq(1).rename('fcloss0018')\n",
    "    FC00 = gfcImage.select(['treecover2000']).gte(20)\n",
    "    FC18 = FC00.subtract(FCL0018).rename('FC18')\n",
    "    FC18Area = FC18.multiply(ee.Image.pixelArea()).rename('FC18Area')\n",
    "    forestSum = FC18Area.reduceRegions(\n",
    "        collection= ee.FeatureCollection(features),\n",
    "        reducer= ee.Reducer.sum(),\n",
    "        scale= 500\n",
    "        #maxPixels= 1e9\n",
    "    )\n",
    "\n",
    "    return forestSum.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeTexture(features, JSONob):\n",
    "    ee.Initialize()\n",
    "    s1 = (ee.ImageCollection('COPERNICUS/S1_GRD').filterBounds(JSONob.clippingBox['geometry']).filterMetadata('instrumentMode', 'equals', 'IW')\n",
    "    .select(['VV', 'angle']))\n",
    "\n",
    "    se1 = s1.select(0).filterDate('2019-01-01', '2019-07-31').median()\n",
    "\n",
    "    textureMean = se1.reduceRegions(\n",
    "        collection= ee.FeatureCollection(features),\n",
    "        reducer= ee.Reducer.mean(),\n",
    "        scale= 10\n",
    "        #maxPixels= 1e9\n",
    "    )\n",
    "    return textureMean.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeSlope(features):\n",
    "   \n",
    "    ee.Initialize()\n",
    "    srtm = ee.Image(\"CSP/ERGo/1_0/Global/SRTM_mTPI\")\n",
    "    slope = ee.Terrain.slope(srtm).rename('slope')\n",
    "    slopeMean = slope.reduceRegions(\n",
    "        collection= ee.FeatureCollection(features),\n",
    "        reducer= ee.Reducer.mean(),\n",
    "        scale= 270\n",
    "        #maxPixels= 1e9\n",
    "    )\n",
    "    return slopeMean.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only return 2019 for now\n",
    "def eeVIIRSNTL(features):\n",
    "    \n",
    "    ee.Initialize()\n",
    "    viirs = ee.ImageCollection(\"NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG\")\n",
    "\n",
    "    #2019 viirs\n",
    "    viirs_2019_mean = viirs.select('avg_rad').filterDate('2019-01-01','2019-07-01').mean().rename('viirs_2019_mean')\n",
    "    viirs_2019_median = viirs.select('avg_rad').filterDate('2019-01-01','2019-07-01').median().rename('viirs_2019_median')\n",
    "    viirs_2019_max = viirs.select('avg_rad').filterDate('2019-01-01','2019-07-01').max().rename('viirs_2019_max')\n",
    "\n",
    "    VIIRS2019 = viirs_2019_mean.addBands(viirs_2019_median).addBands(viirs_2019_max)\n",
    "\n",
    "    stats2019 = VIIRS2019.reduceRegions(\n",
    "        collection= ee.FeatureCollection(features),\n",
    "        reducer= ee.Reducer.sum(),\n",
    "        scale= 500\n",
    "        #maxPixels= 1e9\n",
    "    )\n",
    "\n",
    "   \n",
    "    return stats2019.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeIndices(features, JSONob):\n",
    "   \n",
    "    ee.Initialize()\n",
    "    #Map the function over one year of data and take the median.\n",
    "    #Load Sentinel-2 TOA reflectance data.\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S2')\n",
    "        .filterDate('2019-01-01', '2020-07-31')\n",
    "        #Pre-filter to get less cloudy granules.\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 50)))\n",
    "\n",
    "    composite = collection.median().clip(ee.Feature(JSONob.clippingBox))#.int()  \n",
    "    ndbi = composite.normalizedDifference(['B11','B8']).rename('ndbi')\n",
    "    ndvi = composite.normalizedDifference(['B8','B4']).rename('ndvi')\n",
    "    savi = composite.expression(\n",
    "        '1.5 * (NIR-RED) / (NIR+RED+0.5)', {\n",
    "          'NIR': composite.select('B8'),\n",
    "          'RED': composite.select('B4')\n",
    "    }).rename('savi').float()\n",
    "    urbanIndex = composite.normalizedDifference(['B12','B8']).rename('ui')\n",
    "    sen2AllIndices = ndvi.addBands(ndbi).addBands(savi).addBands(urbanIndex)\n",
    "    #Calculate Indices\n",
    "    finalIndices = sen2AllIndices.reduceRegions(\n",
    "    collection= ee.FeatureCollection(features),\n",
    "    reducer= ee.Reducer.sum(),\n",
    "    scale= 10,\n",
    "    )\n",
    "    return finalIndices.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculating features over polygons</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridJSON = Features(filename)\n",
    "gridPolys=gridJSON.getPolygons()\n",
    "gridDF = gridJSON.createGeoDataFrame()\n",
    "gridFeatures = gridJSON.features\n",
    "gridBounds = gridJSON.getBounds()\n",
    "gridClip = gridJSON.getClippingBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project grid to UTM zone for accurate area calculations\n",
    "gridDFProjected = gridJSON.projectGeoDataFrame(gridDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load a raster and perform a zonal statistic over a set of polygons\n",
    "#polys = list of polygons to perform stats over\n",
    "#stat = string denoting which statistic to calculate\n",
    "#path = raster file path\n",
    "#nodata = raster value that should be treated as no data\n",
    "def zonalStats(polys, stat, path, nodata):\n",
    "    raster = rasterio.open(path)\n",
    "    trans = raster.transform\n",
    "    band = raster.read(1)\n",
    "    stats = zonal_stats(polys, band, affine = trans, stats=[stat], nodata=nodata)\n",
    "    toAdd = [x[stat] for x in stats]\n",
    "    #free memory\n",
    "    band = []\n",
    "    return toAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# processing regular grid JSON\n",
    "###############################################\n",
    "\n",
    "popToAddFB = zonalStats(gridPolys, 'sum', fbPath, np.nan)\n",
    "print(\"done fb\")\n",
    "popToAddWP = zonalStats(gridPolys, 'sum', wpPath, -99999.0)\n",
    "print(\"done wp\")\n",
    "popToAddWSF = zonalStats(gridPolys, 'count', wsfPath, 0)\n",
    "print(\"done wsf\")\n",
    "\n",
    "print(\"done grid ingest and population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google earth engine can only process 5000 features at a time. Split into 4000 feature chunks to be safe and avoid errors\n",
    "gridFeatures = gridJSON.features\n",
    "nFeatures = len(gridFeatures)\n",
    "batchSize = 4000\n",
    "nCalcs = int(math.floor(nFeatures/batchSize) + 1)\n",
    "\n",
    "sections = list(range(0,batchSize*nCalcs,batchSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = gridFeatures[section::]\n",
    "    else:\n",
    "        toProcess = gridFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        forestJSON = eeForest(toProcess)\n",
    "    else:\n",
    "        forestJSON['features'] = forestJSON['features'] + eeForest(toProcess)['features']\n",
    "\n",
    "print(len(forestJSON['features']))\n",
    "print(\"done grid forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = gridFeatures[section::]\n",
    "    else:\n",
    "        toProcess = gridFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        textureJSON = eeTexture(toProcess, gridJSON)\n",
    "    else:\n",
    "        textureJSON['features'] = textureJSON['features'] + eeTexture(toProcess, gridJSON)['features']\n",
    "\n",
    "print(len(textureJSON['features']))\n",
    "print(\"done grid texture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = gridFeatures[section::]\n",
    "    else:\n",
    "        toProcess = gridFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        slopeJSON = eeSlope(toProcess)\n",
    "    else:\n",
    "        slopeJSON['features'] = slopeJSON['features'] + eeSlope(toProcess)['features']\n",
    "\n",
    "print(len(slopeJSON['features']))\n",
    "print(\"done grid slope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = gridFeatures[section::]\n",
    "    else:\n",
    "        toProcess = gridFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        VIIRS2019JSON = eeVIIRSNTL(toProcess)\n",
    "    else:\n",
    "        VIIRS2019JSON['features'] = VIIRS2019JSON['features'] + eeVIIRSNTL(toProcess)['features']\n",
    "\n",
    "print(len(VIIRS2019JSON['features']))\n",
    "print(\"done grid VIIRS NTL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = gridFeatures[section::]\n",
    "    else:\n",
    "        toProcess = gridFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        indicesJSON = eeIndices(toProcess, gridJSON)\n",
    "    else:\n",
    "        indicesJSON['features'] = indicesJSON['features'] + eeIndices(toProcess, gridJSON)['features']\n",
    "\n",
    "print(len(indicesJSON['features']))\n",
    "print(\"done grid indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridFootprints = ox.footprints_from_polygon(gridJSON.bbox)\n",
    "print(\"done getting footprints\")\n",
    "avg_longitudeGrid = (gridBounds[0] + gridBounds[2])/2\n",
    "print(avg_longitudeGrid)\n",
    "utm_zoneGrid = int(math.floor((avg_longitudeGrid + 180) / 6.) + 1)\n",
    "utm_crsGrid = f'+proj=utm +zone={utm_zoneGrid} +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "print(\"projecting\")\n",
    "# project the GeoDataFrame to the UTM CRS\n",
    "gridFootprintsProjected = gridFootprints.to_crs(utm_crsGrid)\n",
    "print(\"generating spatial index\")\n",
    "gridFootprintsSindex = gridFootprintsProjected.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridAreas = []\n",
    "for i,row in enumerate(gridDFProjected.geometry):\n",
    "    gridAreas.append(returnClippedArea(gridFootprintsSindex, gridFootprintsProjected, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnFeatureVals(featureString, variable):\n",
    "    return [x['properties'][featureString] for x in variable['features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridFeatureDict = {\n",
    "    'ndbi':returnFeatureVals('ndbi', indicesJSON),\n",
    "    'ndvi':returnFeatureVals('ndvi', indicesJSON),\n",
    "    'savi':returnFeatureVals('savi', indicesJSON),\n",
    "    'ui':returnFeatureVals('ui', indicesJSON),\n",
    "    'viirs':returnFeatureVals('viirs_2019_max', VIIRS2019JSON),\n",
    "    'slope':returnFeatureVals('mean', slopeJSON),\n",
    "    'texture':returnFeatureVals('mean', textureJSON),\n",
    "    'forest':returnFeatureVals('sum', forestJSON),\n",
    "    'popFB':popToAddFB,\n",
    "    'popWP':popToAddWP,\n",
    "    'popWSF':popToAddWSF,\n",
    "    'area':gridAreas\n",
    "}\n",
    "\n",
    "gridDF = pd.DataFrame.from_dict(gridFeatureDict)\n",
    "gridGeoDF = gpd.GeoDataFrame(gridDF,crs = 4326, geometry=gridJSON.polygons)\n",
    "gridGeoDF = gridGeoDF.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridFeatureDF = gridGeoDF[['ndbi','ndvi','savi','ui','viirs','slope','texture','forest','popFB','popWP','popWSF']]\n",
    "gridTargetDF = gridGeoDF['area']\n",
    "#round output to 3 decimal places to reduce file size\n",
    "gridGeoDF=gridGeoDF.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridGeoDF.to_file(outputFilename, driver=\"GeoJSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
