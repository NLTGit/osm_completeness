{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creating OSM Completeness Model</h1>\n",
    "\n",
    "<p>This Jupyter notebook takes as input training polygons produced from the <a href=\"https://observablehq.com/d/176fbd0640a04220\">Mapbox OSM Training Set Creation</a> Observable notebook. To use this notebook, you'll also need to sign up for a <a href=\"https://earthengine.google.com/new_signup/\"> Google Earth Engine Account</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import geojson\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import shape, box, mapping, Point, Polygon\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterstats import zonal_stats, utils\n",
    "import gdal, gdalconst\n",
    "from scipy.cluster.vq import kmeans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>These are all variables that need to be set before running the notebook</h2>\n",
    "<p>Download <a href =\"https://data.humdata.org/search?organization=facebook&q=high%20resolution%20population&ext_page_size=25&sort=score%20desc%2C%20if(gt(last_modified%2Creview_date)%2Clast_modified%2Creview_date)%20desc\">Facebook population data</a> and/or <a href=\"https://www.worldpop.org/geodata/listing?id=29\">WorldPop</a> data for your area of interest and insert the paths into the cell below. Facebook data is not available for every country, and the World Settlement Footprint (WSF) dataset is not publicly available. If not available, references to those datasets will have to be commented out.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this file path to the location of the json data downloaded from the OSM Training Set Observable notebook\n",
    "trainingJSON = Features(\"path_to_observable_file.json\")\n",
    "\n",
    "#Facebook and WorldPop data paths\n",
    "fbPath = \"path_to_fb_raster\"\n",
    "wpPath = \"path_to_worldpop_raster\"\n",
    "\n",
    "#World settlement footprint path if available\n",
    "wsfPath = \"path_to_wsf_raster\"\n",
    "\n",
    "#define location to save trained model\n",
    "filename = \"path_to_model.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for GeoJSON feature collections. To complete analysis need two:\n",
    "#1. training data from Observable notebook\n",
    "#2. grid data that wil be run through trained model and output\n",
    "class Features:\n",
    "    \n",
    "    def __init__(self, filename=''):\n",
    "        self.filename = filename\n",
    "        self.polygons = []\n",
    "        \n",
    "        if filename=='':\n",
    "            return\n",
    "        \n",
    "        with open(filename) as f:\n",
    "            self.data = json.load(f)\n",
    "        try:\n",
    "            self.features = self.data['features']\n",
    "        except:\n",
    "            print(\"JSON file did not have features array\")\n",
    "            \n",
    "    def getPolygons(self):\n",
    "        for feature in self.features:\n",
    "            g = shape(feature['geometry']).buffer(0)\n",
    "            self.polygons.append(g)\n",
    "        print(len(self.polygons))\n",
    "        return self.polygons\n",
    "    \n",
    "    #used if creating own grid, not reading JSON file in __init__\n",
    "    def setData(self, df):\n",
    "        self.data = mapping(df.geometry)\n",
    "        #don't need bbox for anything so delete it to reduce output file size\n",
    "        del self.data['bbox']\n",
    "        for feature in self.data['features']:\n",
    "            del feature['bbox']\n",
    "        self.features = self.data['features']\n",
    "    \n",
    "    def projectGeoDataFrame(self, *args):\n",
    "        if len(args) > 0:\n",
    "            df = args[0]\n",
    "        else:\n",
    "            df = self.geoDataFrame\n",
    "        \n",
    "        avg_longitude = (self.bounds[0] + self.bounds[2])/2\n",
    "        utm_zone = int(math.floor((avg_longitude + 180) / 6.) + 1)\n",
    "        utm_crs = f'+proj=utm +zone={utm_zone} +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "   \n",
    "        # project the GeoDataFrame to the UTM CRS\n",
    "        self.geoDataFrameProjected = df.to_crs(utm_crs)\n",
    "        \n",
    "        return self.geoDataFrameProjected\n",
    "    \n",
    "    def createGeoDataFrame(self):\n",
    "        self.geoDataFrame = gpd.GeoDataFrame({\n",
    "            'geometry': self.polygons\n",
    "        })\n",
    "        self.geoDataFrame.crs = \"EPSG:4326\"\n",
    "        return self.geoDataFrame\n",
    "        \n",
    "    def getBounds(self):\n",
    "        self.bounds = self.geoDataFrame.total_bounds\n",
    "        return self.bounds\n",
    "    \n",
    "    def getClippingBox(self):\n",
    "        if not hasattr(self, 'bounds'):\n",
    "            self.getBounds()\n",
    "        bbox = box(self.bounds[0], self.bounds[1], self.bounds[2], self.bounds[3])\n",
    "        self.bbox = bbox\n",
    "        self.clippingBox = {'type': 'Feature', 'properties': {}, 'geometry': mapping(bbox)}\n",
    "        return self.clippingBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to work with tiff datasets\n",
    "class Raster:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "    def dataFromBounds(self, band, bounds):\n",
    "        with rasterio.open(self.filename) as src:\n",
    "            self.noData = src.nodatavals[0]\n",
    "            window = rasterio.windows.from_bounds(bounds[0],bounds[1],bounds[2],bounds[3], src.transform)\n",
    "            self.boundsData = src.read(band, window=window)\n",
    "            self.boundsTransform = src.window_transform(window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The series of cells containing functions beginning with <b>ee</b> use the Google Earth Engine API</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeForest(features):\n",
    "    ee.Initialize()\n",
    "    gfcImage = ee.Image(\"UMD/hansen/global_forest_change_2018_v1_6\")\n",
    "    FCL0018 = gfcImage.select(['loss']).eq(1).rename('fcloss0018')\n",
    "    FC00 = gfcImage.select(['treecover2000']).gte(20)\n",
    "    FC18 = FC00.subtract(FCL0018).rename('FC18')\n",
    "    FC18Area = FC18.multiply(ee.Image.pixelArea()).rename('FC18Area')\n",
    "    forestSum = FC18Area.reduceRegions(\n",
    "        collection= ee.FeatureCollection(features),\n",
    "        reducer= ee.Reducer.sum(),\n",
    "        scale= 500\n",
    "        #maxPixels= 1e9\n",
    "    )\n",
    "\n",
    "    return forestSum.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeTexture(features, JSONob):\n",
    "    ee.Initialize()\n",
    "    s1 = (ee.ImageCollection('COPERNICUS/S1_GRD').filterBounds(JSONob.clippingBox['geometry']).filterMetadata('instrumentMode', 'equals', 'IW')\n",
    "    .select(['VV', 'angle']))\n",
    "\n",
    "    se1 = s1.select(0).filterDate('2019-01-01', '2019-07-31').median()\n",
    "\n",
    "    textureMean = se1.reduceRegions(\n",
    "        collection= ee.FeatureCollection(features),\n",
    "        reducer= ee.Reducer.mean(),\n",
    "        scale= 10\n",
    "        #maxPixels= 1e9\n",
    "    )\n",
    "    return textureMean.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeSlope(features):\n",
    "    ee.Initialize()\n",
    "    srtm = ee.Image(\"CSP/ERGo/1_0/Global/SRTM_mTPI\")\n",
    "    slope = ee.Terrain.slope(srtm).rename('slope')\n",
    "    slopeMean = slope.reduceRegions(\n",
    "        collection= ee.FeatureCollection(features),\n",
    "        reducer= ee.Reducer.mean(),\n",
    "        scale= 270\n",
    "        #maxPixels= 1e9\n",
    "    )\n",
    "    return slopeMean.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only return 2019 \n",
    "def eeVIIRSNTL(features):\n",
    "    ee.Initialize()\n",
    "    viirs = ee.ImageCollection(\"NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG\")\n",
    "\n",
    "    #2019 viirs\n",
    "    viirs_2019_mean = viirs.select('avg_rad').filterDate('2019-01-01','2019-07-01').mean().rename('viirs_2019_mean')\n",
    "    viirs_2019_median = viirs.select('avg_rad').filterDate('2019-01-01','2019-07-01').median().rename('viirs_2019_median')\n",
    "    viirs_2019_max = viirs.select('avg_rad').filterDate('2019-01-01','2019-07-01').max().rename('viirs_2019_max')\n",
    "\n",
    "    VIIRS2019 = viirs_2019_mean.addBands(viirs_2019_median).addBands(viirs_2019_max)\n",
    "\n",
    "    stats2019 = VIIRS2019.reduceRegions(\n",
    "        collection= ee.FeatureCollection(features),\n",
    "        reducer= ee.Reducer.sum(),\n",
    "        scale= 500\n",
    "        #maxPixels= 1e9\n",
    "    )\n",
    "\n",
    "    return stats2019.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeIndices(features, JSONob):\n",
    "   \n",
    "    ee.Initialize()\n",
    "    #Map the function over one year of data and take the median.\n",
    "    #Load Sentinel-2 TOA reflectance data.\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S2')\n",
    "        .filterDate('2019-01-01', '2020-07-31')\n",
    "        #Pre-filter to get less cloudy granules.\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 50)))\n",
    "\n",
    "    composite = collection.median().clip(ee.Feature(JSONob.clippingBox))#.int()  \n",
    "    ndbi = composite.normalizedDifference(['B11','B8']).rename('ndbi')\n",
    "    ndvi = composite.normalizedDifference(['B8','B4']).rename('ndvi')\n",
    "    savi = composite.expression(\n",
    "        '1.5 * (NIR-RED) / (NIR+RED+0.5)', {\n",
    "          'NIR': composite.select('B8'),\n",
    "          'RED': composite.select('B4')\n",
    "    }).rename('savi').float()\n",
    "    urbanIndex = composite.normalizedDifference(['B12','B8']).rename('ui')\n",
    "    sen2AllIndices = ndvi.addBands(ndbi).addBands(savi).addBands(urbanIndex)\n",
    "    #Calculate Indices\n",
    "    finalIndices = sen2AllIndices.reduceRegions(\n",
    "    collection= ee.FeatureCollection(features),\n",
    "    reducer= ee.Reducer.sum(),\n",
    "    scale= 10,\n",
    "    )\n",
    "    return finalIndices.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load a raster and perform a zonal statistic over a set of polygons\n",
    "#polys = list of polygons to perform stats over\n",
    "#stat = string denoting which statistic to calculate\n",
    "#path = raster file path\n",
    "#nodata = raster value that should be treated as no data\n",
    "def zonalStats(polys, stat, path, nodata):\n",
    "    raster = rasterio.open(path)\n",
    "    trans = raster.transform\n",
    "    band = raster.read(1)\n",
    "    stats = zonal_stats(polys, band, affine = trans, stats=[stat], nodata=nodata)\n",
    "    toAdd = [x[stat] for x in stats]\n",
    "    #free memory\n",
    "    band = []\n",
    "    return toAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return area of building footprints in cell\n",
    "def returnClippedArea(spatialIndex, buildingDF, clippingPoly):\n",
    "    possible_matches_index = list(spatialIndex.intersection(clippingPoly.bounds))\n",
    "    possible_matches = buildingDF.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(clippingPoly)]\n",
    "    if (precise_matches.empty):\n",
    "        return 0\n",
    "    preciseClip = gpd.clip(precise_matches, clippingPoly)\n",
    "    return (preciseClip.area).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return length of line in grid cell (clippingPoly)\n",
    "def returnClippedLength(spatialIndex, DF, clippingPoly):\n",
    "    possible_matches_index = list(spatialIndex.intersection(clippingPoly.bounds))\n",
    "    possible_matches = DF.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(clippingPoly)]\n",
    "    if (precise_matches.empty):\n",
    "        return 0\n",
    "    preciseClip = gpd.clip(precise_matches, clippingPoly)\n",
    "    return (preciseClip.length).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return number of road intersections in grid cell\n",
    "def returnClippedPoints(spatialIndex, DF, clippingPoly):\n",
    "    possible_matches_index = list(spatialIndex.intersection(clippingPoly.bounds))\n",
    "    possible_matches = DF.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(clippingPoly)]\n",
    "    if (precise_matches.empty):\n",
    "        return 0\n",
    "    preciseClip = gpd.clip(precise_matches, clippingPoly)\n",
    "    return len(preciseClip.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculating features over training polygons</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# processing training data JSON\n",
    "###############################################\n",
    "\n",
    "#order these 6 lines are called is important\n",
    "trainingPolys = trainingJSON.getPolygons()\n",
    "trainingDF = trainingJSON.createGeoDataFrame()\n",
    "trainingFeatures = trainingJSON.features\n",
    "trainingBounds = trainingJSON.getBounds()\n",
    "trainingClip = trainingJSON.getClippingBox()\n",
    "trainingDFProjected = trainingJSON.projectGeoDataFrame(trainingDF)\n",
    "\n",
    "trainingPopToAddFB = zonalStats(trainingPolys, 'sum', fbPath, np.nan)\n",
    "print(\"done fb\")\n",
    "trainingPopToAddWP = zonalStats(trainingPolys, 'sum', wpPath, -99999.0)\n",
    "print(\"done wp\")\n",
    "trainingPopToAddWSF = zonalStats(trainingPolys, 'count', wsfPath, 0)\n",
    "print(\"done wsf\")\n",
    "\n",
    "print(\"done training ingest and population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Downloading building footprints</h3>\n",
    "<p>The purpose of this cell is to download OSM building footprints. The code clusters the training samples and calculates the convex hull for each cluster. It then downloads the footprints in each hull. The purpose of this approach is to avoid downloading large amounts of building footprints that are not needed. For example, if you collected samples in the northwest portion of a country and the southeast portion of a country then used the bounding box of all samples to determine the OSM data downloaded, you would be downloading data for the entire country. In that same situation, k-means clustering with 2 clusters should download footprints for each cluster only.</p>\n",
    "\n",
    "<p>To create two clusters change the kmeans2 line to:</p>\n",
    "\n",
    "`centroid, label = kmeans2(centroids, 2, minit='++')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get centroid of each cell\n",
    "trainingDF[\"centroid\"] = trainingDF.centroid\n",
    "\n",
    "#get centroids in x,y form\n",
    "centroids = []\n",
    "for p in trainingDF.centroid:\n",
    "    centroids.append([p.x,p.y])\n",
    "    \n",
    "#k means cluster so that we don't grab buildings over a larger\n",
    "#area than needed later. This creates 10 clusters\n",
    "centroid, label = kmeans2(centroids, 10, minit='++')\n",
    "\n",
    "centroidIdxs = list(set(label))\n",
    "\n",
    "trainingDF[\"label\"] = label\n",
    "trainingAreas = []\n",
    "oIndices = []\n",
    "\n",
    "#loop through each cluster\n",
    "for idx in centroidIdxs:\n",
    "    print(idx)\n",
    "    subdf = trainingDF.loc[trainingDF['label'] == idx]\n",
    "    print(len(subdf.index))\n",
    "    #convex hull of cluster\n",
    "    subHullPoly = subdf.unary_union.convex_hull\n",
    "    #downloading data using convex hull as polygon boundary\n",
    "    subTrainingFootprints = ox.footprints_from_polygon(subHullPoly)\n",
    "    subBounds = subdf.total_bounds\n",
    "    #convert to UTM for accurate area calculations\n",
    "    avg_longitude = (subBounds[0] + subBounds[2])/2\n",
    "    utm_zone = int(math.floor((avg_longitude + 180) / 6.) + 1)\n",
    "    utm_crs = f'+proj=utm +zone={utm_zone} +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "    print(\"projecting\")\n",
    "    subdfProjected = subdf.to_crs(utm_crs)\n",
    "    subTrainingFootprintsProjected = subTrainingFootprints.to_crs(utm_crs)\n",
    "    subTrainingFootprintsSindex = subTrainingFootprintsProjected.sindex\n",
    "    #loop through cell polygons\n",
    "    for i,row in subdfProjected.iterrows():\n",
    "        oIndices.append(i)\n",
    "        trainingAreas.append(returnClippedArea(subTrainingFootprintsSindex, subTrainingFootprintsProjected, row.geometry))\n",
    "#should be equal to number of training polygons        \n",
    "print(len(oIndices))\n",
    "print(len(trainingAreas))\n",
    "areasDF = pd.DataFrame.from_dict({\n",
    "    \"index\":oIndices,\n",
    "    \"area\":trainingAreas\n",
    "})\n",
    "\n",
    "#merge dataframes back together to align areas with original dataframe\n",
    "trainingDF2 = pd.merge(areasDF, trainingDF, right_index=True, left_on='index')\n",
    "trainingDF2 = trainingDF2.sort_values('index')\n",
    "trainingAreas = trainingDF2['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google earth engine can only process 5000 features at a time. Split into 4000 feature chunks to be safe and avoid errors\n",
    "#I have still occasionally received errors with 4000. In that case I just keep lowering by 500 until the errors go away\n",
    "trainingFeatures = trainingJSON.features\n",
    "nFeatures = len(trainingFeatures)\n",
    "batchSize = 4000\n",
    "nCalcs = int(math.floor(nFeatures/batchSize) + 1)\n",
    "\n",
    "sections = list(range(0,batchSize*nCalcs,batchSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingForestJSON = eeForest(trainingFeatures)\n",
    "#print(\"done training forest\")\n",
    "\n",
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = trainingFeatures[section::]\n",
    "    else:\n",
    "        toProcess = trainingFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        trainingForestJSON = eeForest(toProcess)\n",
    "    else:\n",
    "        trainingForestJSON['features'] = trainingForestJSON['features'] + eeForest(toProcess)['features']\n",
    "\n",
    "print(len(trainingForestJSON['features']))\n",
    "print(\"done training forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingTextureJSON = eeTexture(trainingFeatures, trainingJSON)\n",
    "#print(\"done training texture\")\n",
    "\n",
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = trainingFeatures[section::]\n",
    "    else:\n",
    "        toProcess = trainingFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        trainingTextureJSON = eeTexture(toProcess, trainingJSON)\n",
    "    else:\n",
    "        trainingTextureJSON['features'] = trainingTextureJSON['features'] + eeTexture(toProcess, trainingJSON)['features']\n",
    "\n",
    "print(len(trainingTextureJSON['features']))\n",
    "print(\"done training texture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingSlopeJSON = eeSlope(trainingFeatures)\n",
    "#print(\"done training slope\")\n",
    "\n",
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = trainingFeatures[section::]\n",
    "    else:\n",
    "        toProcess = trainingFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        trainingSlopeJSON = eeSlope(toProcess)\n",
    "    else:\n",
    "        trainingSlopeJSON['features'] = trainingSlopeJSON['features'] + eeSlope(toProcess)['features']\n",
    "\n",
    "print(len(trainingSlopeJSON['features']))\n",
    "print(\"done training slope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingVIIRS2019JSON = eeVIIRSNTL(trainingFeatures)\n",
    "#print(\"done training VIIRS NTL\")\n",
    "\n",
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = trainingFeatures[section::]\n",
    "    else:\n",
    "        toProcess = trainingFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        trainingVIIRS2019JSON = eeVIIRSNTL(toProcess)\n",
    "    else:\n",
    "        trainingVIIRS2019JSON['features'] = trainingVIIRS2019JSON['features'] + eeVIIRSNTL(toProcess)['features']\n",
    "\n",
    "print(len(trainingVIIRS2019JSON['features']))\n",
    "print(\"done training VIIRS NTL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingIndicesJSON = eeIndices(trainingFeatures, trainingJSON)\n",
    "#print(\"done training indices\")\n",
    "\n",
    "for section in sections:\n",
    "    print(section)\n",
    "    if section == sections[-1]:\n",
    "        toProcess = trainingFeatures[section::]\n",
    "    else:\n",
    "        toProcess = trainingFeatures[section:section+batchSize]\n",
    "        \n",
    "    if section == 0:\n",
    "        trainingIndicesJSON = eeIndices(toProcess, trainingJSON)\n",
    "    else:\n",
    "        trainingIndicesJSON['features'] = trainingIndicesJSON['features'] + eeIndices(toProcess, trainingJSON)['features']\n",
    "\n",
    "print(len(trainingIndicesJSON['features']))\n",
    "print(\"done training indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to get values from objects returned from earth engine\n",
    "def returnFeatureVals(featureString, variable):\n",
    "    return [x['properties'][featureString] for x in variable['features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFeatureDict = {\n",
    "    'ndbi':returnFeatureVals('ndbi', trainingIndicesJSON),\n",
    "    'ndvi':returnFeatureVals('ndvi', trainingIndicesJSON),\n",
    "    'savi':returnFeatureVals('savi', trainingIndicesJSON),\n",
    "    'ui':returnFeatureVals('ui', trainingIndicesJSON),\n",
    "    'viirs':returnFeatureVals('viirs_2019_max', trainingVIIRS2019JSON),\n",
    "    'slope':returnFeatureVals('mean', trainingSlopeJSON),\n",
    "    'texture':returnFeatureVals('mean', trainingTextureJSON),\n",
    "    'forest':returnFeatureVals('sum', trainingForestJSON),\n",
    "    'popFB':trainingPopToAddFB,\n",
    "    'popWP':trainingPopToAddWP,\n",
    "    'popWSF':trainingPopToAddWSF,\n",
    "    'area':trainingAreas\n",
    "}\n",
    "\n",
    "trainingDF = pd.DataFrame.from_dict(trainingFeatureDict)\n",
    "trainingGeoDF = gpd.GeoDataFrame(trainingDF,crs = 4326, geometry=trainingJSON.polygons)\n",
    "trainingGeoDF = trainingGeoDF.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot correlation matrix\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.heatmap(trainingGeoDF.corr(), annot=True, fmt=\".2f\",cmap='RdBu_r', center=0, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFeatureDF = trainingGeoDF[['ndbi','ndvi','savi','ui','viirs','slope','texture','forest','popFB','popWP','popWSF']]\n",
    "trainingTargetDF = trainingGeoDF['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trainingFeatureDF, trainingTargetDF, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit with default settings for reference (70/30 training/test split, no cross validation)\n",
    "RandomForest = RandomForestRegressor()\n",
    "RandomForest.fit(X_train, y_train)\n",
    "y_pred = RandomForest.predict(X_test)\n",
    "print(r2_score(y_test,y_pred))\n",
    "ax1 = sns.scatterplot(x=y_test,y=y_pred)\n",
    "ax1.set(ylim=(-1000, 30000))\n",
    "ax1.set(xlim=(-1000, 30000))\n",
    "print(mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit with optimized settings (takes a few minutes)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_random.best_params_)\n",
    "best_random = rf_random.best_estimator_\n",
    "y_pred = best_random.predict(X_test)\n",
    "print(r2_score(y_test,y_pred))\n",
    "ax = sns.scatterplot(x=y_test,y=y_pred)\n",
    "ax.set(ylim=(-1000, 30000))\n",
    "ax.set(xlim=(-1000, 30000))\n",
    "print(mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(best_random, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
